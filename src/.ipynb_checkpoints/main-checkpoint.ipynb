{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c39b2b68-f43f-42d5-9dab-10ee04016ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from utils.bert import BERTFakeNewsClassifier\n",
    "from utils.rnn import RNNFakeNewsClassifier, build_vocabulary, preprocess_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a078609d-34ee-4da4-9d58-7cb0eee23976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets():\n",
    "    \"\"\"Load and combine both true and fake news datasets\"\"\"\n",
    "    print(\"Loading datasets...\")\n",
    "    \n",
    "    dataset_files = {\n",
    "        'true': '../dataset/dataset/datasets/True.csv',\n",
    "        'false': '../dataset/dataset/datasets/Fake.csv'\n",
    "    }\n",
    "    \n",
    "    dataframes = []\n",
    "    \n",
    "    for label_name, file_path in dataset_files.items():\n",
    "        try:\n",
    "            print(f\"Loading {file_path}...\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['binary_label'] = 0 if label_name == 'true' else 1\n",
    "            df['source_file'] = label_name\n",
    "            print(f\"   Loaded: {len(df)} {label_name} news samples\")\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            raise FileNotFoundError(f\"Error loading {file_path}: {str(e)}\")\n",
    "    \n",
    "    if len(dataframes) == 0:\n",
    "        raise FileNotFoundError(\"No dataset files found\")\n",
    "    \n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Find text column\n",
    "    text_column = None\n",
    "    for col in ['text', 'title']:\n",
    "        if col in combined_df.columns:\n",
    "            text_column = col\n",
    "            break\n",
    "    \n",
    "    if text_column is None:\n",
    "        text_columns = combined_df.select_dtypes(include=['object']).columns\n",
    "        if len(text_columns) > 0:\n",
    "            text_column = text_columns[0]\n",
    "    \n",
    "    if text_column is None:\n",
    "        raise ValueError(\"No text column found in dataset\")\n",
    "    \n",
    "    print(f\"\\nDataset Summary:\")\n",
    "    print(f\"Total samples: {len(combined_df)}\")\n",
    "    print(f\"Text column: '{text_column}'\")\n",
    "    \n",
    "    distribution = combined_df['binary_label'].value_counts()\n",
    "    for label, count in distribution.items():\n",
    "        label_name = \"Fake News\" if label == 1 else \"Real News\"\n",
    "        percentage = (count / len(combined_df)) * 100\n",
    "        print(f\"  {label} ({label_name}): {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    return combined_df, text_column\n",
    "\n",
    "def create_balanced_dataset(df, text_column, max_samples_per_class=None):\n",
    "    \"\"\"Create a balanced dataset\"\"\"\n",
    "    print(\"\\nBalancing dataset...\")\n",
    "    \n",
    "    class_counts = df['binary_label'].value_counts()\n",
    "    min_samples = class_counts.min()\n",
    "    \n",
    "    if max_samples_per_class:\n",
    "        min_samples = min(max_samples_per_class, min_samples)\n",
    "    \n",
    "    print(f\"Using {min_samples} samples per class...\")\n",
    "    \n",
    "    balanced_dfs = []\n",
    "    for label in df['binary_label'].unique():\n",
    "        class_df = df[df['binary_label'] == label].sample(n=min_samples, random_state=42)\n",
    "        balanced_dfs.append(class_df)\n",
    "    \n",
    "    balanced_df = pd.concat(balanced_dfs, ignore_index=True)\n",
    "    \n",
    "    texts = balanced_df[text_column].fillna('').tolist()\n",
    "    labels = balanced_df['binary_label'].tolist()\n",
    "    \n",
    "    print(f\"Balanced dataset created: {len(texts)} total samples\")\n",
    "    balanced_counts = balanced_df['binary_label'].value_counts()\n",
    "    for label, count in balanced_counts.items():\n",
    "        label_name = \"Fake News\" if label == 1 else \"Real News\"\n",
    "        print(f\"  {label} ({label_name}): {count}\")\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "def split_data(texts, labels, test_size=0.2, val_size=0.1):\n",
    "    \"\"\"Split data into train, validation, and test sets\"\"\"\n",
    "    print(f\"\\nSplitting data...\")\n",
    "    \n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        texts, labels, test_size=test_size+val_size, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    val_ratio = val_size / (test_size + val_size)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=1-val_ratio, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {len(X_train)} samples\")\n",
    "    print(f\"Validation set: {len(X_val)} samples\")\n",
    "    print(f\"Test set: {len(X_test)} samples\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de926ee8-c142-4ad6-bb93-04b405ef6b11",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35ca8c74-f4df-4f11-9ea3-5e8d00307283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert_model(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    \"\"\"Train BERT model\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training BERT Model\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    bert_classifier = BERTFakeNewsClassifier(model_name='distilbert-base-uncased')\n",
    "    bert_classifier.prepare_data(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    \n",
    "    bert_classifier.train(epochs=3, batch_size=16, learning_rate=2e-5)\n",
    "    \n",
    "    test_loss, test_acc, predictions, true_labels, report, cm = bert_classifier.test_model()\n",
    "    \n",
    "    print(f\"BERT Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    return bert_classifier, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42069f0-a6c8-45e0-b230-606fdc04cf94",
   "metadata": {},
   "source": [
    "# RNN + LSTM + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a693fe18-73db-448b-9c96-c8e05263d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn_model(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    \"\"\"Train RNN model\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training RNN Model\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Build vocabulary from training data\n",
    "    processed_texts = [preprocess_text(text) for text in X_train]\n",
    "    vocab_to_idx, vocab = build_vocabulary(processed_texts, min_freq=3, max_vocab_size=15000)\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    rnn_classifier = RNNFakeNewsClassifier(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=128,\n",
    "        hidden_dim=256,\n",
    "        num_layers=2,\n",
    "        dropout=0.4,\n",
    "        rnn_type='LSTM',\n",
    "        bidirectional=True\n",
    "    )\n",
    "    \n",
    "    rnn_classifier.prepare_data(X_train, X_val, X_test, y_train, y_val, y_test, vocab_to_idx)\n",
    "    \n",
    "    rnn_classifier.train(num_epochs=10, learning_rate=0.001, batch_size=32)\n",
    "    \n",
    "    test_loss, test_acc, predictions, true_labels, report, cm = rnn_classifier.test_model()\n",
    "    \n",
    "    print(f\"RNN Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    return rnn_classifier, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a42e58f-51ca-4ba2-a588-317b2fbee6e1",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73c2589d-38bc-419f-9b0a-46a39f6ca221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison(bert_classifier, rnn_classifier):\n",
    "    \"\"\"Plot comparison of both models' training progress\"\"\"\n",
    "    print(\"\\nGenerating comparison plots...\")\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('BERT vs RNN Comparison', fontsize=16)\n",
    "    \n",
    "    # Plot 1: Training Loss Comparison\n",
    "    axes[0,0].plot(bert_classifier.train_losses, label='BERT', marker='o', linewidth=2)\n",
    "    axes[0,0].plot(rnn_classifier.train_losses, label='RNN', marker='s', linewidth=2)\n",
    "    axes[0,0].set_title('Training Loss Comparison')\n",
    "    axes[0,0].set_xlabel('Epoch')\n",
    "    axes[0,0].set_ylabel('Loss')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Validation Loss Comparison\n",
    "    axes[0,1].plot(bert_classifier.val_losses, label='BERT', marker='o', linewidth=2)\n",
    "    axes[0,1].plot(rnn_classifier.val_losses, label='RNN', marker='s', linewidth=2)\n",
    "    axes[0,1].set_title('Validation Loss Comparison')\n",
    "    axes[0,1].set_xlabel('Epoch')\n",
    "    axes[0,1].set_ylabel('Loss')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Validation Accuracy Comparison\n",
    "    axes[1,0].plot(bert_classifier.val_accuracies, label='BERT', marker='o', linewidth=2)\n",
    "    axes[1,0].plot(rnn_classifier.val_accuracies, label='RNN', marker='s', linewidth=2)\n",
    "    axes[1,0].set_title('Validation Accuracy Comparison')\n",
    "    axes[1,0].set_xlabel('Epoch')\n",
    "    axes[1,0].set_ylabel('Accuracy')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Training Accuracy Comparison (for RNN only, BERT doesn't track this)\n",
    "    if hasattr(rnn_classifier, 'train_accuracies') and rnn_classifier.train_accuracies:\n",
    "        axes[1,1].plot(rnn_classifier.train_accuracies, label='RNN Training', marker='s', linewidth=2)\n",
    "        axes[1,1].plot(rnn_classifier.val_accuracies, label='RNN Validation', marker='o', linewidth=2)\n",
    "        axes[1,1].set_title('RNN Training vs Validation Accuracy')\n",
    "        axes[1,1].set_xlabel('Epoch')\n",
    "        axes[1,1].set_ylabel('Accuracy')\n",
    "        axes[1,1].legend()\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1,1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Model comparison plot displayed\")\n",
    "\n",
    "def plot_final_accuracy_comparison(bert_acc, rnn_acc):\n",
    "    \"\"\"Plot final test accuracy comparison\"\"\"\n",
    "    models = ['BERT', 'RNN']\n",
    "    accuracies = [bert_acc, rnn_acc]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    bars = plt.bar(models, accuracies, color=['#1f77b4', '#ff7f0e'], alpha=0.8)\n",
    "    plt.title('Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add accuracy values on top of bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.show()\n",
    "    print(\"Final accuracy comparison displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2086efb3-92d8-4093-bdf2-6ee0125341bc",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31f6e338-52ed-4677-a028-7fade98550c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predictions(bert_classifier, rnn_classifier, X_test, y_test):\n",
    "    \"\"\"Test both models on sample texts and show predictions\"\"\"\n",
    "    print(\"\\nTesting sample predictions...\")\n",
    "    \n",
    "    sample_texts = X_test[:5]\n",
    "    sample_labels = y_test[:5]\n",
    "    \n",
    "    bert_predictions, bert_probs = bert_classifier.predict(sample_texts)\n",
    "    rnn_predictions, rnn_probs = rnn_classifier.predict(sample_texts)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Sample Predictions Comparison\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for i, (text, true_label) in enumerate(zip(sample_texts, sample_labels)):\n",
    "        bert_pred = bert_predictions[i]\n",
    "        rnn_pred = rnn_predictions[i]\n",
    "        \n",
    "        bert_conf = max(bert_probs[i]) * 100\n",
    "        rnn_conf = max(rnn_probs[i]) * 100\n",
    "        \n",
    "        true_name = \"FAKE\" if true_label == 1 else \"REAL\"\n",
    "        bert_name = \"FAKE\" if bert_pred == 1 else \"REAL\"\n",
    "        rnn_name = \"FAKE\" if rnn_pred == 1 else \"REAL\"\n",
    "        \n",
    "        bert_correct = \"✓\" if bert_pred == true_label else \"✗\"\n",
    "        rnn_correct = \"✓\" if rnn_pred == true_label else \"✗\"\n",
    "        \n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"Text: {text[:100]}...\")\n",
    "        print(f\"True Label: {true_name}\")\n",
    "        print(f\"BERT: {bert_name} ({bert_conf:.1f}%) {bert_correct}\")\n",
    "        print(f\"RNN:  {rnn_name} ({rnn_conf:.1f}%) {rnn_correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6e365d7-3dba-464b-b8b4-b5778c30321f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Loading ../dataset/dataset/datasets/True.csv...\n",
      "   Loaded: 21417 true news samples\n",
      "Loading ../dataset/dataset/datasets/Fake.csv...\n",
      "   Loaded: 23481 false news samples\n",
      "\n",
      "Dataset Summary:\n",
      "Total samples: 44898\n",
      "Text column: 'text'\n",
      "  1 (Fake News): 23481 (52.3%)\n",
      "  0 (Real News): 21417 (47.7%)\n",
      "\n",
      "Balancing dataset...\n",
      "Using 5000 samples per class...\n",
      "Balanced dataset created: 10000 total samples\n",
      "  0 (Real News): 5000\n",
      "  1 (Fake News): 5000\n",
      "\n",
      "Splitting data...\n",
      "Training set: 6999 samples\n",
      "Validation set: 1000 samples\n",
      "Test set: 2001 samples\n",
      "\n",
      "==================================================\n",
      "Training BERT Model\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "BERT Epoch 1/3: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 438/438 [04:42<00:00,  1.55it/s, loss=0.0012]\n",
      "BERT Epoch 2/3: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 438/438 [04:52<00:00,  1.50it/s, loss=0.0002]\n",
      "BERT Epoch 3/3: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 438/438 [04:49<00:00,  1.51it/s, loss=0.0002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Test Accuracy: 1.0000\n",
      "\n",
      "==================================================\n",
      "Training RNN Model\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing texts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6999/6999 [00:08<00:00, 791.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 15000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RNN Epoch 1/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:18<00:00, 11.87it/s, loss=0.0105]\n",
      "RNN Epoch 2/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:18<00:00, 11.99it/s, loss=0.0023]\n",
      "RNN Epoch 3/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:18<00:00, 12.06it/s, loss=0.0046]\n",
      "RNN Epoch 4/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:18<00:00, 12.07it/s, loss=0.0022]\n",
      "RNN Epoch 5/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:18<00:00, 12.02it/s, loss=0.0090]\n",
      "RNN Epoch 6/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:18<00:00, 11.65it/s, loss=0.0159]\n",
      "RNN Epoch 7/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:18<00:00, 12.06it/s, loss=0.0007]\n",
      "RNN Epoch 8/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:19<00:00, 11.30it/s, loss=0.0008]\n",
      "RNN Epoch 9/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:18<00:00, 11.95it/s, loss=0.0233]\n",
      "/root/deepLearning/src/utils/rnn.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load('./result/best_rnn_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Test Accuracy: 0.9975\n",
      "\n",
      "Generating comparison plots...\n",
      "Model comparison plot saved as 'model_comparison.png'\n",
      "Final accuracy comparison saved as 'final_accuracy_comparison.png'\n",
      "\n",
      "Testing sample predictions...\n",
      "\n",
      "============================================================\n",
      "Sample Predictions Comparison\n",
      "============================================================\n",
      "\n",
      "Sample 1:\n",
      "Text: WASHINGTON (Reuters) - Protesters at Howard University chanted and booed on Friday through James Com...\n",
      "True Label: REAL\n",
      "BERT: REAL (100.0%) ✓\n",
      "RNN:  REAL (99.9%) ✓\n",
      "\n",
      "Sample 2:\n",
      "Text: BENGHAZI, Libya (Reuters) - Armed men blew up a pipeline pumping crude oil to Es Sider port on Tuesd...\n",
      "True Label: REAL\n",
      "BERT: REAL (100.0%) ✓\n",
      "RNN:  REAL (99.8%) ✓\n",
      "\n",
      "Sample 3:\n",
      "Text: If it weren t for the whiney phrase,  political correctness,  which is really just another phrase fo...\n",
      "True Label: FAKE\n",
      "BERT: FAKE (100.0%) ✓\n",
      "RNN:  FAKE (99.9%) ✓\n",
      "\n",
      "Sample 4:\n",
      "Text: It has been 15 years since the World Trade Center Twin Towers fell. But Republicans and Donald Trump...\n",
      "True Label: FAKE\n",
      "BERT: FAKE (100.0%) ✓\n",
      "RNN:  FAKE (99.8%) ✓\n",
      "\n",
      "Sample 5:\n",
      "Text: Russian interference in our election should immediately disqualify Donald Trump from the presidency....\n",
      "True Label: FAKE\n",
      "BERT: FAKE (100.0%) ✓\n",
      "RNN:  FAKE (99.0%) ✓\n",
      "Final Results Summary \n",
      "\n",
      "BERT Test Accuracy: 1.0000 (100.00%)\n",
      "RNN Test Accuracy:  0.9975 (99.75%)\n",
      "BERT outperforms RNN by 0.25 percentage points\n",
      "\n",
      "Files generated:\n",
      "- model_comparison.png: Training progress comparison\n",
      "- final_accuracy_comparison.png: Test accuracy comparison\n",
      "- best_rnn_model.pth: Best RNN model weights\n",
      "- bert_fake_news_model/: BERT model directory\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# Load datasets\n",
    "df, text_column = load_datasets()   \n",
    "    \n",
    "# Create balanced dataset\n",
    "texts, labels = create_balanced_dataset(df, text_column, max_samples_per_class=5000)\n",
    "    \n",
    "# Split data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(texts, labels)\n",
    "    \n",
    "# Train both models\n",
    "bert_classifier, bert_accuracy = train_bert_model(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "rnn_classifier, rnn_accuracy = train_rnn_model(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    \n",
    "# Generate comparison plots\n",
    "plot_model_comparison(bert_classifier, rnn_classifier)\n",
    "plot_final_accuracy_comparison(bert_accuracy, rnn_accuracy)\n",
    "    \n",
    "# Test sample predictions\n",
    "test_predictions(bert_classifier, rnn_classifier, X_test, y_test)\n",
    "    \n",
    "print(\"Final Results Summary \\n\")\n",
    "print(f\"BERT Test Accuracy: {bert_accuracy:.4f} ({bert_accuracy*100:.2f}%)\")\n",
    "print(f\"RNN Test Accuracy:  {rnn_accuracy:.4f} ({rnn_accuracy*100:.2f}%)\")\n",
    "    \n",
    "if bert_accuracy > rnn_accuracy:\n",
    "    diff = (bert_accuracy - rnn_accuracy) * 100\n",
    "    print(f\"BERT outperforms RNN by {diff:.2f} percentage points\")\n",
    "elif rnn_accuracy > bert_accuracy:\n",
    "    diff = (rnn_accuracy - bert_accuracy) * 100\n",
    "    print(f\"RNN outperforms BERT by {diff:.2f} percentage points\")\n",
    "else:\n",
    "    print(\"Both models achieved the same accuracy\")\n",
    "    \n",
    "print(\"\\nFiles generated:\")\n",
    "print(\"- model_comparison.png: Training progress comparison\")\n",
    "print(\"- final_accuracy_comparison.png: Test accuracy comparison\")\n",
    "print(\"- best_rnn_model.pth: Best RNN model weights\")\n",
    "print(\"- bert_fake_news_model/: BERT model directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad8d619-994b-481d-8781-1e0978ce46b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
